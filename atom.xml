<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>WillFeng&#39;s Blog</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2020-10-13T08:22:22.553Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>WillFeng</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>ex1线性回归</title>
    <link href="http://example.com/2020/10/12/ex1%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    <id>http://example.com/2020/10/12/ex1%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</id>
    <published>2020-10-12T12:57:02.000Z</published>
    <updated>2020-10-13T08:22:22.553Z</updated>
    
    <content type="html"><![CDATA[<h1 id="单变量回归问题"><a href="#单变量回归问题" class="headerlink" title="单变量回归问题"></a>单变量回归问题</h1><p>​       我们将使用一个变量实现线性回归，根据城市人口数量，预测开小吃店的利润，数据在ex1data1.txt里，第一列是城市人口数量，第二列是该城市小吃店利润。</p><a id="more"></a><h2 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h2><p>利用pandas读取数据，并显示前5行数据检查是否读取成功。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.read_csv(<span class="string">&#x27;ex1data1.txt&#x27;</span>, header=<span class="literal">None</span>, names=[<span class="string">&#x27;Population&#x27;</span>, <span class="string">&#x27;Profit&#x27;</span>])</span><br><span class="line">print(data.head())</span><br></pre></td></tr></table></figure><div class="table-container"><table><thead><tr><th></th><th style="text-align:center">Population</th><th>Profit</th></tr></thead><tbody><tr><td>0</td><td style="text-align:center">6.1101</td><td>17.5920</td></tr><tr><td>1</td><td style="text-align:center">5.5277</td><td>9.1302</td></tr><tr><td>2</td><td style="text-align:center">8.5186</td><td>13.6620</td></tr><tr><td>3</td><td style="text-align:center">7.0032</td><td>11.8540</td></tr><tr><td>4</td><td style="text-align:center">5.8598</td><td>6.8233</td></tr></tbody></table></div><h2 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h2><p>第一步我们需要对数据进行可视化，初步判断数据之间是否存在线性关系，这对后面的分析十分重要。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.scatter(x=data[<span class="string">&#x27;Population&#x27;</span>], y=data[<span class="string">&#x27;Profit&#x27;</span>], label=<span class="string">&#x27;data&#x27;</span>, marker=<span class="string">&#x27;.&#x27;</span>, color=<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2020/10/12/ex1%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/Figure_1.png" alt></p><h2 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h2><p>根据吴恩达课程所讲，我们需要在x的第一列加上一列x_0=1,初始化初值为0的theta。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">data.insert(<span class="number">0</span>, <span class="string">&#x27;ones&#x27;</span>, <span class="number">1</span>)  <span class="comment"># 在第0列插入1，表头为ones </span></span><br><span class="line">x = data.iloc[:, <span class="number">0</span>:<span class="number">-1</span>]  <span class="comment"># 切片工具[几行到几行, 几列到几列]</span></span><br><span class="line">y = data.iloc[:, <span class="number">-1</span>:]</span><br><span class="line">x, y = np.matrix(x.values), np.matrix(y.values)</span><br><span class="line">theta = np.matrix(np.zeros(x.shape[<span class="number">1</span>]))  <span class="comment"># 初始化theta</span></span><br></pre></td></tr></table></figure><h2 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h2><p>采用回归问题中常用的平方误差代价函数。</p><script type="math/tex; mode=display">J（\Theta）= \frac{1}{2m}\times\sum_{i=1}^m(h_\Theta(x^i)-y^i)^2</script><p>其中</p><script type="math/tex; mode=display">h_\Theta(x)=\Theta_0+\Theta_1\times x_1</script><p>注意在前面初始化的时候添加了一列x_0,故</p><script type="math/tex; mode=display">h_\Theta(x)=x\cdot\Theta^T</script><p>我们的目标是求出能够最小化J(θ)的θ，这样预测值才能更加接近y，即:</p><script type="math/tex; mode=display">argmin_\Theta=J(\Theta)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 计算代价函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost_function</span>(<span class="params">x_, y_, theta_</span>):</span></span><br><span class="line">    ans = np.power(x_.dot(theta_.T)-y_, <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> np.sum(ans)/(<span class="number">2</span>*x_.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">print(cost_function(x, y, theta))</span><br></pre></td></tr></table></figure><p>得出的结果应为theta=32.072733877455676。</p><h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><script type="math/tex; mode=display">\Theta_j:=\Theta_j-\alpha\times\frac{\partial J(\Theta)}{\partial \Theta_j}        :=\Theta_j-\frac{1}{m}\times \sum_{i=1}^m(h_\Theta(x^i)-y^i)x^i</script><p>其中α为learning rate,用来控制梯度下降的幅度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_descent</span>(<span class="params">x_, y_, theta_, alpha_, inters_</span>):</span></span><br><span class="line">    temp = np.matrix(np.zeros(theta.shape))</span><br><span class="line">    nums = int(theta_.shape[<span class="number">1</span>])</span><br><span class="line">    cost = np.zeros(inters_)</span><br><span class="line">    <span class="keyword">for</span> inter <span class="keyword">in</span> range(inters_):</span><br><span class="line">        ans = x_.dot(theta_.T)-y_</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(nums):</span><br><span class="line">            temp[<span class="number">0</span>, i] = theta_[<span class="number">0</span>, i] - alpha_*np.sum(np.multiply(ans, x_[:, i]))/len(x_)</span><br><span class="line">        theta_ = temp</span><br><span class="line">        cost[inter] = cost_function(x_, y_, theta_)</span><br><span class="line">    <span class="keyword">return</span> theta_, cost</span><br></pre></td></tr></table></figure><h2 id="预测函数"><a href="#预测函数" class="headerlink" title="预测函数"></a>预测函数</h2><p>设置α以及梯度下降的迭代次数，对θ进行求解。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置参数</span></span><br><span class="line">alpha = <span class="number">0.01</span></span><br><span class="line">inters = <span class="number">1500</span></span><br><span class="line">g, cost = gradient_descent(x, y, theta, alpha, inters)</span><br><span class="line"><span class="comment"># 画出预测函数</span></span><br><span class="line">x_test = np.linspace(data[<span class="string">&#x27;Population&#x27;</span>].min(), data[<span class="string">&#x27;Population&#x27;</span>].max(), data.shape[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># linspace(start,end,num=50)从间隔取出50个数</span></span><br><span class="line">predict = g[<span class="number">0</span>, <span class="number">0</span>]+g[<span class="number">0</span>, <span class="number">1</span>]*x_test</span><br><span class="line">plt.scatter(x=data[<span class="string">&#x27;Population&#x27;</span>], y=data[<span class="string">&#x27;Profit&#x27;</span>], label=<span class="string">&#x27;data&#x27;</span>, marker=<span class="string">&#x27;.&#x27;</span>, color=<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line">plt.plot(x_test, predict, c=<span class="string">&#x27;r&#x27;</span>, label=<span class="string">&#x27;predict&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Population&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Profit&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>得出最后的预测函数如下图所示。</p><p><img src="/2020/10/12/ex1%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/Figure_2.png" alt></p><h1 id="多变量回归问题"><a href="#多变量回归问题" class="headerlink" title="多变量回归问题"></a>多变量回归问题</h1><p>​      在这一部分中，我们将使用多个变量实现线性回归以预测房屋价格。假设你正在出售房屋，并且想知道一个好的市场价格。一种方法是首先收集最近有关出售房屋的信息，并建立房屋价格模型。<br>​    文件ex1data2.txt包含某地区房屋价格的训练集。第一列是房屋的大小（以平方英尺为单位），第二列是卧室的数量，第三列是房屋的价格。</p><h2 id="读取数据-1"><a href="#读取数据-1" class="headerlink" title="读取数据"></a>读取数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data2 = pd.read_csv(<span class="string">&#x27;ex1data2.txt&#x27;</span>, header=<span class="literal">None</span>, names=[<span class="string">&#x27;Size&#x27;</span>, <span class="string">&#x27;Bedrooms&#x27;</span>, <span class="string">&#x27;Price&#x27;</span>])</span><br><span class="line">print(data2.head())</span><br></pre></td></tr></table></figure><p>显示前5行数据如下，</p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">Size</th><th style="text-align:center">Bedrooms</th><th style="text-align:center">Price</th></tr></thead><tbody><tr><td style="text-align:center">0</td><td style="text-align:center">2104</td><td style="text-align:center">3</td><td style="text-align:center">399900</td></tr><tr><td style="text-align:center">1</td><td style="text-align:center">1600</td><td style="text-align:center">3</td><td style="text-align:center">329900</td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">2400</td><td style="text-align:center">3</td><td style="text-align:center">369000</td></tr><tr><td style="text-align:center">3</td><td style="text-align:center">1416</td><td style="text-align:center">2</td><td style="text-align:center">232000</td></tr><tr><td style="text-align:center">4</td><td style="text-align:center">3000</td><td style="text-align:center">4</td><td style="text-align:center">539900</td></tr></tbody></table></div><p>这时我们需要进行均值归一化让数据统一量级，让梯度下降算法更快的收敛。均值归一化就是将每个特征的值减去平均值再除以标准差。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data2 = (data2-data2.mean())/data2.std()</span><br><span class="line">print(data2.head())</span><br></pre></td></tr></table></figure><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">Size</th><th style="text-align:center">Bedrooms</th><th style="text-align:center">Price</th></tr></thead><tbody><tr><td style="text-align:center">0</td><td style="text-align:center">0.130010</td><td style="text-align:center">-0.223675</td><td style="text-align:center">0.475747</td></tr><tr><td style="text-align:center">1</td><td style="text-align:center">-0.504190</td><td style="text-align:center">-0.223675</td><td style="text-align:center">-0.084074</td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">0.502476</td><td style="text-align:center">-0.223675</td><td style="text-align:center">0.228626</td></tr><tr><td style="text-align:center">3</td><td style="text-align:center">-0.735723</td><td style="text-align:center">-1.537767</td><td style="text-align:center">-0.867025</td></tr><tr><td style="text-align:center">4</td><td style="text-align:center">1.257476</td><td style="text-align:center">1.090417</td><td style="text-align:center">1.595389</td></tr></tbody></table></div><h2 id="数据处理-1"><a href="#数据处理-1" class="headerlink" title="数据处理"></a>数据处理</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">data2.insert(<span class="number">0</span>, <span class="string">&#x27;ones&#x27;</span>, <span class="number">1</span>)</span><br><span class="line">x2 = data2.iloc[:, <span class="number">0</span>:<span class="number">-1</span>]  </span><br><span class="line">y2 = data2.iloc[:, <span class="number">-1</span>:]</span><br><span class="line">x2, y2 = np.matrix(x2.values), np.matrix(y2.values)</span><br><span class="line">theta2 = np.matrix(np.zeros(x2.shape[<span class="number">1</span>]))  </span><br></pre></td></tr></table></figure><h2 id="梯度下降-1"><a href="#梯度下降-1" class="headerlink" title="梯度下降"></a>梯度下降</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">g2, cost2 &#x3D; gradient_descent(x2, y2, theta2, alpha, inters)</span><br><span class="line">print(g2)</span><br></pre></td></tr></table></figure><p>输出结果为[[-1.10892383e-16  8.84042349e-01 -5.24551809e-02]]    </p><h2 id="特征方程"><a href="#特征方程" class="headerlink" title="特征方程"></a>特征方程</h2><p>正规方程是通过求解下面的方程来找出使得代价函数最小的参数的：</p><script type="math/tex; mode=display">\frac{\partial J(\Theta_j)}{\partial\Theta_j}=0</script><p>通过特征方程，解得：</p><script type="math/tex; mode=display">\Theta=(X^TX)^{-1}X^TY</script><p>python实现代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normal_eqn</span>(<span class="params">x_, y_</span>):</span>  <span class="comment"># 正规方程</span></span><br><span class="line">    <span class="keyword">return</span> np.linalg.inv(x_.T.dot(x_)).dot(x_.T).dot(y_)</span><br></pre></td></tr></table></figure><p>梯度下降与正规方程的比较：</p><p>梯度下降：需要选择学习率α，需要多次迭代，当特征数量n大时也能较好适用，适用于各种类型的模型</p><p>正规方程：不需要选择学习率α，一次计算得出，需要计算逆矩阵，如果特征数量n较大则运算代价大。</p><p>下面使用特征方程求解θ与梯度下降的结果进行对比。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(normal_eqn(x2, y2))</span><br></pre></td></tr></table></figure><p>结果为如下，可以看出与梯度下降所得的结果很相近。</p><p>[[-9.36750677e-17]<br> [ 8.84765988e-01]<br> [-5.31788197e-02]]</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;单变量回归问题&quot;&gt;&lt;a href=&quot;#单变量回归问题&quot; class=&quot;headerlink&quot; title=&quot;单变量回归问题&quot;&gt;&lt;/a&gt;单变量回归问题&lt;/h1&gt;&lt;p&gt;​       我们将使用一个变量实现线性回归，根据城市人口数量，预测开小吃店的利润，数据在ex1data1.txt里，第一列是城市人口数量，第二列是该城市小吃店利润。&lt;/p&gt;</summary>
    
    
    
    <category term="吴恩达机器学习练习" scheme="http://example.com/categories/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BB%83%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="线性回归" scheme="http://example.com/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>regression</title>
    <link href="http://example.com/2020/09/28/regression/"/>
    <id>http://example.com/2020/09/28/regression/</id>
    <published>2020-09-28T11:10:13.000Z</published>
    <updated>2020-09-28T14:09:19.912Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>（转载）简单线性回归-基于高斯核函数</title>
    <link href="http://example.com/2020/09/27/GaussianRegression/"/>
    <id>http://example.com/2020/09/27/GaussianRegression/</id>
    <published>2020-09-27T13:14:58.000Z</published>
    <updated>2020-09-27T14:55:30.896Z</updated>
    
    <content type="html"><![CDATA[<h2 id="高斯过程"><a href="#高斯过程" class="headerlink" title="高斯过程"></a>高斯过程</h2><p>高斯过程 Gaussian Processes 是概率论和数理统计中随机过程的一种，是多元高斯分布的扩展，被应用于机器学习、信号处理等领域。本文对高斯过程进行公式推导、原理阐述、可视化以及代码实现，介绍了以高斯过程为基础的高斯过程回归 Gaussian Process Regression 基本原理、超参优化、高维输入等问题。</p><a id="more"></a><ul><li><p>一元高斯分布公式<br>其概率密度公式如下：</p><script type="math/tex; mode=display">p(x)=\frac{1}{σ\sqrt{2\times\pi}}exp(-\frac{(x-μ)^2}{2σ^2})</script></li><li><p>核函数(协方差函数)<br>核函数是一个高斯过程的核心，核函数决定了一个高斯过程的性质。核函数在高斯过程中起的作用是生成一个协方差矩阵（相关系数矩阵），衡量任意两个点之间的“距离”。最常用的一个核函数为高斯核函数，也成为径向基函数 RBF。其基本形式如下。其中 σ 和 h 是高斯核的超参数。</p><script type="math/tex; mode=display">K(x_i,x_j)=σ^2exp(-\frac{-||x_i-x_j||^2}{2h^2})</script></li></ul><h1 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h1><h2 id="1-定义核函数"><a href="#1-定义核函数" class="headerlink" title="1.定义核函数"></a>1.定义核函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义核函数</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">kernel</span>(<span class="params">self, x1, x2</span>):</span></span><br><span class="line">       m,n = x1.shape[<span class="number">0</span>], x2.shape[<span class="number">0</span>]</span><br><span class="line">       dist_matrix = np.zeros((m,n), dtype=float)</span><br><span class="line">       <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">           <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">               dist_matrix[i][j] = np.sum((x1[i]-x2[j])**<span class="number">2</span>)</span><br><span class="line">       <span class="keyword">return</span> np.exp(<span class="number">-0.5</span>/self.h**<span class="number">2</span>*dist_matrix)</span><br></pre></td></tr></table></figure><h2 id="2-主要代码"><a href="#2-主要代码" class="headerlink" title="2.主要代码"></a>2.主要代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入相关库</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># 定义高斯过程类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GPR</span>:</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, h</span>):</span></span><br><span class="line">       self.is_fit = <span class="literal">False</span></span><br><span class="line">       self.train_x, self.train_y = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">       self.h = h</span><br><span class="line">       </span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, x, y</span>):</span></span><br><span class="line">       self.train_x = np.asarray(x)</span><br><span class="line">       self.train_y = np.asarray(y)</span><br><span class="line">       self.is_fit = <span class="literal">True</span></span><br><span class="line">       </span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, x</span>):</span></span><br><span class="line">       <span class="keyword">if</span> <span class="keyword">not</span> self.is_fit:</span><br><span class="line">           print(<span class="string">&quot;Sorry! GPR Model can&#x27;t fit!&quot;</span>)</span><br><span class="line">           <span class="keyword">return</span></span><br><span class="line">       </span><br><span class="line">       x = np.asarray(x)</span><br><span class="line">       kff = self.kernel(x,x)</span><br><span class="line">       kyy = self.kernel(self.train_x, self.train_x)</span><br><span class="line">       kfy = self.kernel(x, self.train_x)</span><br><span class="line">       kyy_inv =  np.linalg.inv(kyy + <span class="number">1e-8</span>*np.eye(len(self.train_x)))</span><br><span class="line">       </span><br><span class="line">       mu = kfy.dot(kyy_inv).dot(self.train_y)</span><br><span class="line">       <span class="keyword">return</span> mu</span><br><span class="line">       </span><br><span class="line">       </span><br><span class="line">   <span class="comment"># 定义核函数</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">kernel</span>(<span class="params">self, x1, x2</span>):</span></span><br><span class="line">       m,n = x1.shape[<span class="number">0</span>], x2.shape[<span class="number">0</span>]</span><br><span class="line">       dist_matrix = np.zeros((m,n), dtype=float)</span><br><span class="line">       <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">           <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">               dist_matrix[i][j] = np.sum((x1[i]-x2[j])**<span class="number">2</span>)</span><br><span class="line">       <span class="keyword">return</span> np.exp(<span class="number">-0.5</span>/self.h**<span class="number">2</span>*dist_matrix)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创造训练集</span></span><br><span class="line">train_x = np.arange(<span class="number">0</span>,<span class="number">10</span>).reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line">train_y = np.cos(train_x) + train_x</span><br><span class="line"><span class="comment"># 制造槽点</span></span><br><span class="line">train_y = train_y + np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=train_x.shape)</span><br><span class="line"><span class="comment"># 显示训练集的分布</span></span><br><span class="line">plt.figure()</span><br><span class="line">plt.scatter(train_x, train_y, label=<span class="string">&quot;train&quot;</span>, c=<span class="string">&quot;red&quot;</span>, marker=<span class="string">&quot;x&quot;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>输出的图：</p><p><img src="/2020/09/27/GaussianRegression/Figure_1.png" alt="1"></p><h2 id="3-学习代码"><a href="#3-学习代码" class="headerlink" title="3.学习代码"></a>3.学习代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建训练集</span></span><br><span class="line">test_x = np.arange(<span class="number">0</span>, <span class="number">10</span>, <span class="number">0.1</span>).reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 针对不同h值得到拟合图像</span></span><br><span class="line">h=<span class="number">0.1</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    gpr = GPR(h)</span><br><span class="line">    gpr.fit(train_x, train_y)</span><br><span class="line">    mu = gpr.predict(test_x)</span><br><span class="line">    test_y = mu.ravel()</span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.title(<span class="string">&quot;h=%.2f&quot;</span>%(h))</span><br><span class="line">    plt.plot(test_x, test_y, label=<span class="string">&quot;predict&quot;</span>)</span><br><span class="line">    plt.scatter(train_x, train_y, label=<span class="string">&quot;train&quot;</span>, c=<span class="string">&quot;red&quot;</span>, marker=<span class="string">&quot;x&quot;</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.show()</span><br><span class="line">    h += <span class="number">0.1</span></span><br></pre></td></tr></table></figure><p>输出图像：</p><p><img src="/2020/09/27/GaussianRegression/Figure_2.png" alt="2"></p><p><img src="/2020/09/27/GaussianRegression/Figure_3.png" alt="2"></p><p><img src="/2020/09/27/GaussianRegression/Figure_4.png" alt="2"></p><p><img src="/2020/09/27/GaussianRegression/Figure_5.png" alt="2"></p><h1 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h1><p>本人是刚接触机器学习的小白，这篇文章让对老师布置的作业更好的理解。文章转载自CSDN（<a href="https://me.csdn.net/Vince_Cheng">Vince_Cheng</a>），<a href="https://blog.csdn.net/Vince_Cheng/article/details/102250958">文章地址</a>转载仅作为本人学习使用的笔记，同时也是我的第一篇博客，相信日后我也能写出原创的文章。感谢这位大佬的分享，侵删！    </p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;高斯过程&quot;&gt;&lt;a href=&quot;#高斯过程&quot; class=&quot;headerlink&quot; title=&quot;高斯过程&quot;&gt;&lt;/a&gt;高斯过程&lt;/h2&gt;&lt;p&gt;高斯过程 Gaussian Processes 是概率论和数理统计中随机过程的一种，是多元高斯分布的扩展，被应用于机器学习、信号处理等领域。本文对高斯过程进行公式推导、原理阐述、可视化以及代码实现，介绍了以高斯过程为基础的高斯过程回归 Gaussian Process Regression 基本原理、超参优化、高维输入等问题。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="线性回归" scheme="http://example.com/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
    <category term="核函数" scheme="http://example.com/tags/%E6%A0%B8%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
</feed>
